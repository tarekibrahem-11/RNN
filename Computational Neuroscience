import numpy as np
def softmax(x):
    shifted_x = x - np.max(x)
    exps = np.exp(shifted_x)
    return exps / np.sum(exps, axis=0)

def tanh(x):
    return np.tanh(x)

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size, seed=1):
        np.random.seed(seed)
        self.hidden_size = hidden_size
        self.Wx = np.random.randn(hidden_size, input_size) * 0.01
        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Wy = np.random.randn(output_size, hidden_size) * 0.01
        self.h_prev = np.zeros((hidden_size, 1))

    def forward(self, inputs):
        hs, ys = {}, {}
        h = self.h_prev

        for t, x_t in enumerate(inputs):
            x = x_t.reshape(-1, 1)
            h = tanh(self.Wx @ x + self.Wh @ h)
            y = softmax(self.Wy @ h)

            hs[t] = h
            ys[t] = y
        
        self.h_prev = h  
        return hs, ys

input_size = 4
hidden_size = 3
output_size = 4

X = [
    np.array([1, 0, 0, 0]),
    np.array([0, 1, 0, 0]),
    np.array([0, 0, 1, 0])
]

rnn = SimpleRNN(input_size, hidden_size, output_size)
hidden_states, predictions = rnn.forward(X)

for t, y in predictions.items():
    print(f"Time step {t}: PredictionÂ {y.ravel()}")
